{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment and run the commands below if imports fail\n#!conda install numpy pandas pytorch torchvision cpuonly -c pytorch -y\n#!pip install matplotlib --upgrade --quiet\n#!pip install torch\n#!pip install torchtext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e'\n# here is an example of sentiment analysis - https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n\n# text related\nfrom torchtext import data\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_name = '05-course-project-text-classification'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's inspect the data\n\n> You can see in the dataset has accompanied code that can help load the data into a dataframe. We use that code snippet to load the initial 1000 rows and inspect data to get an intuition.\n\nIt turns out that there may be latin characters that requires to use `encoding` parameter when reading the csv as you see below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf1 = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', delimiter=',', nrows = None, encoding='latin-1', names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\ndf1.dataframeName = 'training.1600000.processed.noemoticon.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's ensure that we have the right classes that have been [documented here](https://www.kaggle.com/kazanova/sentiment140) *0 = negative, 4 = positive* ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the code from here, let inspect the distrubution of columns that have unique values between 1 and 50. The plot below shows the target or column-0 from the input data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a TensorDataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are not interested in anything other than the text and the target columns, let's\ndf_input = df1[['target','text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df1[['target','text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_input.shape, df_input.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text) \n    stop_words = set(stopwords.words('english'))\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]           \n    return ' '.join(filtered_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use nltk to remove stop words, remove, newlines, extra spaces, and punctuations\nfrom nltk.tokenize import RegexpTokenizer\n\ndef preprocess_text(texts):\n    return_text = []\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text = remove_stopwords(texts)\n    text = tokenizer.tokenize(text)\n    return_text = text\n    print('return_text:', return_text, type(return_text))\n    return ' '.join(return_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 100\nfor idx, t in df1.iterrows():\n    #print(preprocess_text(t['text']),\";\", type(preprocess_text(t['text'])))\n    if len(t['text'].split()) > size:\n        size = len(t['text'].split())\nprint(size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess data and prepare tensors\nLet's define instances of `Field` for our `text` and `target` that can contain the Vocabolary and their corresponding numeric representations as [detailed here](https://torchtext.readthedocs.io/en/latest/data.html#field). We will use the our function `preprocess_text` to remove stop words, trim spaces, remove special characters. We perform all these steps on the text field that contain the text of the tweets\n\n\nHow do we split the data, the options include subclassing the Dataset class or using the methods detailed here https://torchtext.readthedocs.io/en/latest/datasets.html\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# given the 280 chars limit on tweets and based on the average size of the data, we can set max_document_length to be 100\nmax_document_length = 100\nText = data.Field(preprocessing=preprocess_text, tokenize=\"spacy\", batch_first=True, include_lengths=True, fix_length=max_document_length)\nTarget = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's associate the fields in the dataset to the definitions above - `Text` and `Target`\n#fields = [('text', Text), ('target', Target)]\nfields = [('text', Text)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the dataset**\n\n sentiment140 dataset has 1600000 samples, we need to split that dataset into tran and validation sets. ~~We will be using `random_split` from the package `torch.utils.data` to accomplish this~~ I am not sure why this results in an *TypeError: expected string or bytes-like object* error.\n \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from torchtext import data\n# some_train_dataset = data.TabularDataset.splits(\n#     path='../input/sentiment140',\n#     train='training.1600000.processed.noemoticon.csv',\n#     format='csv',\n#     fields=fields,\n#     skip_header=False\n# )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code copied from https://averdones.github.io/reading-tabular-data-with-pytorch-and-training-a-multilayer-perceptron/\n\nfrom torch.utils.data import Dataset\n\nclass TwitterSentimentDataset(Dataset):\n    \"\"\"Twitter sentiment dataset.\"\"\"\n\n    def __init__(self, csv_file):\n        \"\"\"Initializes instance of class TwitterSentimentDataset.\n        Args:\n            csv_file (str): Path to the csv file with the students data.\n        \"\"\"\n        df = pd.read_csv(csv_file, delimiter=',', nrows = None, encoding='latin-1', names=[\"target\", \"text\"])\n\n        # Grouping variable names\n        self.categorical = ['text'] # dont think we need all these columns - [ 'id', 'date', 'flag', 'user', 'text']\n        self.target = \"target\"\n\n        # One-hot encoding of categorical variables\n        self.tweet_frame = pd.get_dummies(df['text'], prefix=self.categorical)\n\n        # Save target and predictors\n        self.X = self.tweet_frame.drop(self.target, axis=1)\n        self.y = self.tweet_frame[self.target]\n\n    def __len__(self):\n        return len(self.tweet_frame)\n\n    def __getitem__(self, idx):\n        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n        if isinstance(idx, torch.Tensor):\n            idx = idx.tolist()\n\n        return [self.X.iloc[idx].values, self.y[idx]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\ndataset = TwitterSentimentDataset('../input/sentiment140/training.1600000.processed.noemoticon.csv')\n\n# Split into training and test\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrainset, valset = random_split(dataset, [train_size, val_size])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make splits for data\ntrain, val = datasets.IMDB.splits(Text, Target)\n\n# build the vocabulary\nText.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n\n# make iterator for splits\ntrain_iter, test_iter = data.BucketIterator.splits(\n    (train, val), batch_size=3, device=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(some_train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# val_percent = 0.15 # between 01 and 0.2\n# val_size = int(val_percent * len(df1[['target','text']]))\n# train_size = len(df1[['target','text']]) - val_size\n# train_data, val_data = torch.utils.data.random_split(df1[['text']], [train_size, val_size])\n\n\n\nseed = 46\ntrain_data, val_data = some_train_data.split(split_ratio=0.8, random_state=random.seed(seed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data), len(val_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n\nseed = 42\ntrain_data, valid_data = train_data.split(split_ratio=0.8, random_state=random.seed(seed))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" print(list(torch.utils.data.DataLoader(trainloader_1, num_workers=2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python program to generate WordCloud \n  \n# importing all necessery modules \nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \nimport pandas as pd \n  \ndef make_word_cloud(df):  \n    comment_words = '' \n    stopwords = set(STOPWORDS) \n  \n    # iterate through the csv file \n    for val in df.CONTENT: \n      \n        # typecaste each val to string \n        val = str(val) \n  \n        # split the value \n        tokens = val.split() \n      \n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n      \n        comment_words += \" \".join(tokens)+\" \"\n  \n        wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n  \n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the vocabulary\n\nWe have the train, validation data sets. We also have out \"containers\" or objects defined to hold the vocabolary for Text. We can use the build_vocab on the Text field to build a vocab object using one or more datasets. We can find [more details here](https://torchtext.readthedocs.io/en/latest/data.html#field)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Text.build_vocab(train_data, val_data, max_size=5000)\nLabel.build_vocab(train_data)\nvocab_size = len(Text.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset = TensorDataset(inputs, targets)\n\n#val_percent = 0.15 # between 0.1 and 0.2\n#val_size = int(num_rows * val_percent)\n#train_size = num_rows - val_size\n\n#print(train_size, val_size, len(dataset))\n#train_ds, val_ds = random_split(dataset, [train_size, val_size]) # Use the random_split function to split dataset into 2 parts of the desired length","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This implementation can be split up into the following high level steps\n\n* Preprocess data to remove unwanted characters and tokenize\n* Process input data to build vocabolary and load Glove Embeddings\n* build the model\n    - train \n    - test\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import jovian\njovian.commit(project=project_name, environment=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}