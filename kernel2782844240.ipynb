{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment and run the commands below if imports fail\n#!conda install numpy pandas pytorch torchvision cpuonly -c pytorch -y\n#!pip install matplotlib --upgrade --quiet\n#!pip install torch\n#!pip install torchtext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e'\n# here is an example of sentiment analysis - https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\n\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n\n# text related\nfrom torchtext import data\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_name = '05-course-project-text-classification'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's inspect the data\n\n> You can see in the dataset has accompanied code that can help load the data into a dataframe. We use that code snippet to load the initial 1000 rows and inspect data to get an intuition.\n\nIt turns out that there may be latin characters that requires to use `encoding` parameter when reading the csv as you see below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf1 = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', delimiter=',', nrows = None, encoding='latin-1', names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\ndf1.dataframeName = 'training.1600000.processed.noemoticon.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's ensure that we have the right classes that have been [documented here](https://www.kaggle.com/kazanova/sentiment140) *0 = negative, 4 = positive* ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the code from here, let inspect the distrubution of columns that have unique values between 1 and 50. The plot below shows the target or column-0 from the input data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we need to create PyTorch datasets & data loaders for training & validation. We'll start by creating a TensorDataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are not interested in anything other than the text and the target columns, let's\ndf_input = df1[['target','text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df1[['target','text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_input.shape, df_input.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.parsing.preprocessing import remove_stopwords\n\ntext = \"this is a test sentence, I like cricket and not other sports.\"\nret_val = []\nret_val.append(remove_stopwords(text))\nprint(ret_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n# from nltk.corpus import stopwords\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndef my_remove_stopwords(text):\n    word_tokens = word_tokenize(text) \n#     stop_words = set(stopwords.words('english'))\n#     filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n    filtered_sentence = remove_stopwords(\" \".join(word_tokens))\n    return filtered_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use nltk to remove stop words, remove, newlines, extra spaces, and punctuations\nfrom nltk.tokenize import RegexpTokenizer\nfrom gensim.parsing.preprocessing import remove_stopwords\n\ndef preprocess_text(texts):\n    return_text = []\n#     tokenizer = RegexpTokenizer(r'\\w+')\n    #preprocess_pipeline = data.Pipeline(lambda x: x.decode('latin1').encode('utf8'))\n    for i, text in enumerate(texts):\n        #remove non ascii chars\n        #text = remove_stopwords(text)\n        #text = tokenizer.tokenize(text)\n        \n        return_text.append(remove_stopwords(text))\n    return_text_str = \" \".join(str(x) for x in return_text)\n    # print('return_text:', return_text, type(return_text), return_text_str, type(return_text_str))\n\n    return return_text_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: now that we know 100 is a good size, we dont need to execute this anymore\n# size = 100\n# for idx, t in df1.iterrows():\n#     #print(preprocess_text(t['text']),\";\", type(preprocess_text(t['text'])))\n#     if len(t['text'].split()) > size:\n#         size = len(t['text'].split())\n# print(size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess data and prepare tensors\nLet's define instances of `Field` for our `text` and `target` that can contain the Vocabolary and their corresponding numeric representations as [detailed here](https://torchtext.readthedocs.io/en/latest/data.html#field). We will use the our function `preprocess_text` to remove stop words, trim spaces, remove special characters. We perform all these steps on the text field that contain the text of the tweets\n\n\nHow do we split the data, the options include subclassing the Dataset class or using the methods detailed here https://torchtext.readthedocs.io/en/latest/datasets.html\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# given the 280 chars limit on tweets and based on the average size of the data, we can set max_document_length to be 100\nmax_document_length = 100\nText = data.Field(preprocessing=preprocess_text, tokenize=\"spacy\", batch_first=True, include_lengths=True, fix_length=max_document_length)\nTarget = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's associate the fields in the dataset to the definitions above - `Text` and `Target`\nfields = [('target', Target),('id', None), ('date', None), ('flag', None), ('user', None) ,('text', Text)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torch\n# print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the dataset**\n\n sentiment140 dataset has 1600000 samples, we need to split that dataset into tran and validation sets. ~~We will be using `random_split` from the package `torch.utils.data` to accomplish this~~ I am not sure why this results in an *TypeError: expected string or bytes-like object* error.\n \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: dont want to spend more time on how to write to output, maybe later!\n# infile = '../input/sentiment140/training.1600000.processed.noemoticon.csv'\n# outfile = '../output/training.1600000.processed.noemoticon.vinod.csv'\n# BLOCKSIZE = 1024*1024\n# with open(infile, 'rb') as inf:\n#     with open(outfile, 'w') as ouf:\n#         while True:\n#             data = inf.read(BLOCKSIZE)\n#             if not data: break\n#             converted = data.decode('latin-1').encode('utf-8')\n#             ouf.write(converted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extend TabularDataset so we can set the encoding to latin-1; based on [source here](https://pytorch.org/text/_modules/torchtext/data/dataset.html#TabularDataset)\n\nfrom  torchtext.data.dataset import Dataset\nfrom torchtext.data import Example\nfrom torchtext.utils import download_from_url, unicode_csv_reader\n\nimport io\nimport os\n\nclass MyTabularDataset(Dataset):\n    def __init__(self, path, format, fields, skip_header=False,\n                 csv_reader_params={}, **kwargs):\n        format = format.lower()\n        make_example = {\n            'json': Example.fromJSON, 'dict': Example.fromdict,\n            'tsv': Example.fromCSV, 'csv': Example.fromCSV}[format]\n\n        with io.open(os.path.expanduser(path), encoding=\"latin-1\") as f:\n            if format == 'csv':\n                reader = unicode_csv_reader(f, **csv_reader_params)\n            elif format == 'tsv':\n                reader = unicode_csv_reader(f, delimiter='\\t', **csv_reader_params)\n            else:\n                reader = f\n\n            if format in ['csv', 'tsv'] and isinstance(fields, dict):\n                if skip_header:\n                    raise ValueError('When using a dict to specify fields with a {} file,'\n                                     'skip_header must be False and'\n                                     'the file must have a header.'.format(format))\n                header = next(reader)\n                field_to_index = {f: header.index(f) for f in fields.keys()}\n                make_example = partial(make_example, field_to_index=field_to_index)\n\n            if skip_header:\n                next(reader)\n\n            examples = [make_example(line, fields) for line in reader]\n\n        if isinstance(fields, dict):\n            fields, field_dict = [], fields\n            for field in field_dict.values():\n                if isinstance(field, list):\n                    fields.extend(field)\n                else:\n                    fields.append(field)\n\n        super(MyTabularDataset, self).__init__(examples, fields, **kwargs)\n        #super(MyTabularDataset, self).__init__(self, path, format, fields, **kwargs)\n        \n    def check_split_ratio(split_ratio):\n        valid_ratio = 0.\n        if isinstance(split_ratio, float):\n            # Only the train set relative ratio is provided\n            # Assert in bounds, validation size is zero\n            assert 0. < split_ratio < 1., (\n                \"Split ratio {} not between 0 and 1\".format(split_ratio))\n\n            test_ratio = 1. - split_ratio\n            return (split_ratio, test_ratio, valid_ratio)\n        elif isinstance(split_ratio, list):\n            # A list of relative ratios is provided\n            length = len(split_ratio)\n            assert length == 2 or length == 3, (\n                \"Length of split ratio list should be 2 or 3, got {}\".format(split_ratio))\n\n            # Normalize if necessary\n            ratio_sum = sum(split_ratio)\n            if not ratio_sum == 1.:\n                split_ratio = [float(ratio) / ratio_sum for ratio in split_ratio]\n\n            if length == 2:\n                return tuple(split_ratio + [valid_ratio])\n            return tuple(split_ratio)\n        else:\n            raise ValueError('Split ratio must be float or a list, got {}'\n                             .format(type(split_ratio)))\n\n\n    def stratify(examples, strata_field):\n        # The field has to be hashable otherwise this doesn't work\n        # There's two iterations over the whole dataset here, which can be\n        # reduced to just one if a dedicated method for stratified splitting is used\n        unique_strata = set(getattr(example, strata_field) for example in examples)\n        strata_maps = {s: [] for s in unique_strata}\n        for example in examples:\n            strata_maps[getattr(example, strata_field)].append(example)\n        return list(strata_maps.values())\n\n\n    def rationed_split(examples, train_ratio, test_ratio, val_ratio, rnd):\n        \n        N = len(examples)\n        randperm = rnd(range(N))\n        train_len = int(round(train_ratio * N))\n\n        # Due to possible rounding problems\n        if not val_ratio:\n            test_len = N - train_len\n        else:\n            test_len = int(round(test_ratio * N))\n\n        indices = (randperm[:train_len],  # Train\n                   randperm[train_len:train_len + test_len],  # Test\n                   randperm[train_len + test_len:])  # Validation\n\n        # There's a possibly empty list for the validation set\n        data = tuple([examples[i] for i in index] for index in indices)\n\n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from torchtext import data\n# some_train_dataset = data.TabularDataset.splits(\n#     path='../input/sentiment140',\n#     train='training.1600000.processed.noemoticon.csv',\n#     format='csv',\n#     fields=fields,\n#     skip_header=False\n# )\n#'id', 'date', 'flag', 'user', 'text'\ntrain = MyTabularDataset(\n    path= '../input/sentiment140/training.1600000.processed.noemoticon.csv', format='csv', skip_header=False,fields=fields)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(vars(trainset[2000]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"type(train):\", type(train), vars(train[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( train[0].target, train[0].text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Python program to generate WordCloud \n  \n# # importing all necessery modules \n# from wordcloud import WordCloud, STOPWORDS \n# import matplotlib.pyplot as plt \n# import pandas as pd \n  \n# def make_word_cloud(df):  \n#     comment_words = '' \n#     stopwords = set(STOPWORDS) \n  \n#     # iterate through the csv file \n#     for val in df.CONTENT: \n      \n#         # typecaste each val to string \n#         val = str(val) \n  \n#         # split the value \n#         tokens = val.split() \n      \n#         # Converts each token into lowercase \n#         for i in range(len(tokens)): \n#             tokens[i] = tokens[i].lower() \n      \n#         comment_words += \" \".join(tokens)+\" \"\n  \n#         wordcloud = WordCloud(width = 800, height = 800, \n#                 background_color ='white', \n#                 stopwords = stopwords, \n#                 min_font_size = 10).generate(comment_words) \n  \n#     # plot the WordCloud image                        \n#     plt.figure(figsize = (8, 8), facecolor = None) \n#     plt.imshow(wordcloud) \n#     plt.axis(\"off\") \n#     plt.tight_layout(pad = 0) \n  \n#     plt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the vocabulary\n\nWe have the train, validation data sets. We also have out \"containers\" or objects defined to hold the vocabolary for Text. We can use the build_vocab on the Text field to build a vocab object using one or more datasets. We can find [more details here](https://torchtext.readthedocs.io/en/latest/data.html#field)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n# Split into training and test\n# train_size = int(0.8 * len(train))\n# val_size = len(train) - train_size\n# trainset, valset = random_split(train, [train_size, val_size])\nseed = 42\nmax_vocab_size = 5000 \n\ntrain_data, valid_data = train.split(split_ratio=0.8, random_state=random.seed(seed))\n\n# Text.build_vocab(train_data, valid_data, max_size=max_vocab_size)\n# Target.build_vocab(train_data)\n# vocab_size = len(Text.vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This implementation can be split up into the following high level steps\n\n* Preprocess data to remove unwanted characters and tokenize\n* Process input data to build vocabolary and load Glove Embeddings\n* build the model\n    - train \n    - test\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext import vocab\nfrom torchtext.vocab import GloVe\n# the following results in an error - \n# embeddings = vocab.Vectors('glove.840B.300d.txt', '../input/pickled-glove840b300d-for-10sec-loading/')\n# Text.build_vocab(train_data, valid_data, max_size=max_vocab_size, vectors=embeddings) \n\n# based on documentation here, use the steps detailed https://pytorch.org/text/datasets.html : glove.6B.200d\nText.build_vocab(train_data, vectors=GloVe(name='6B', dim=200))\nTarget.build_vocab(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vars(Text.vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Text.numericalize(([['good','stuff']],2),device=None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is not how we get the embeddings\n# import torch.nn as nn\n# embed_size = 300\n# vocab_size = 5000\n# offsets = [len(train[0].text)]\n# print(offsets)\n# embedding = nn.Embedding(vocab_size, embed_size)\n# embedding(train[0].text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examples from pytorch docs I think\n# word_to_ix = {\"hello\": 0, \"world\": 1}\n# embeds =nn.Embedding(vocab_size, embed_size)  # 2 words in vocab, 5 dimensional embeddings\n# lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n# hello_embed = embeds(lookup_tensor)\n# print(hello_embed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\nprint(\"device: \",device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train the model 4-5 times with different learning rates & for different number of epochs.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-4\nnum_classes = 5 # 0=negative, 4=positive\n\nhidden_l1_size = 128\nhidden_l2_size = 64\nbatch_size=100\nepochs = 10\n\n\ndropout_keep_prob = 0.5\nembedding_size = 300\nmax_document_length = 100  \ndev_size = 0.8 \n\nhidden_size = 128\npool_size = 2\nn_filters = 128\nfilter_sizes = [3, 8]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\ntrain_iterator, valid_iterator = data.BucketIterator.splits((train_data, valid_data),\n        batch_size = batch_size,\n        sort_key = lambda x: len(x.text), sort_within_batch = True,\n        device = device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data loaders, in other words iterators","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### CNN class Implementation\nLet's implement our CNN class that inherits from Module with two fully connected or dense layers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note: credit goes to [galhev](https://github.com/galhev/Neural-Sentiment-Analyzer-for-Modern-Hebrew/blob/master/models/cnn_model.py) as I have heavily borrowed code here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's recall max-pooling ![max-pool](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)","attachments":{},"execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Read more about Conv1D, Conv2D and Conv3D [here](https://medium.com/@xzz201920/conv1d-conv2d-and-conv3d-8a59182c4d6)\n\n\nConv2D used in Image processing where kernel traverses in two dimensions see diagram below\n\n![kernel traversal good diagram for conv2d](https://miro.medium.com/max/1400/0*k0YJHHjTUY1WSsT-.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, n_filters, \n                 filter_sizes, pool_size, hidden_size, num_classes,\n                 dropout):\n        super().__init__()        \n        # initalize embedding with our vocabulary size and embedding size (the number of dimensions for each token)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        #\n        self.convs = nn.ModuleList([nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embed_size))  for fs in filter_sizes])\n        #max pool layer\n        self.max_pool1 = nn.MaxPool1d(pool_size)\n        # ReLU\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(95*n_filters, hidden_size, bias=True)  \n        self.fc2 = nn.Linear(hidden_size, num_classes, bias=True)  \n\n    def forward(self, text, text_lengths):\n        embedded = self.embedding(text)\n        embedded = embedded.unsqueeze(1)  \n#     def forward(embedded):\n        convolution = [conv(embedded) for conv in self.convs]   \n        max1 = self.max_pool1(convolution[0].squeeze()) \n        max2 = self.max_pool1(convolution[1].squeeze())\n        cat = torch.cat((max1, max2), dim=2)      \n        x = cat.view(cat.shape[0], -1) \n        x = self.fc1(self.relu(x))\n        x = self.dropout(x)\n        x = self.fc2(x)  \n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### fit and accuracy functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# def accuracy(outputs, labels):\n#     _, preds = torch.max(outputs, dim=1)\n#     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n# class ImageClassificationBase(nn.Module):\n#     def training_step(self, batch):\n#         images, labels = batch \n#         out = self(images)                  # Generate predictions\n#         loss = F.cross_entropy(out, labels) # Calculate loss\n#         return loss\n    \n#     def validation_step(self, batch):\n#         images, labels = batch \n#         out = self(images)                    # Generate predictions\n#         loss = F.cross_entropy(out, labels)   # Calculate loss\n#         acc = accuracy(out, labels)           # Calculate accuracy\n#         return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n#     def validation_epoch_end(self, outputs):\n#         batch_losses = [x['val_loss'] for x in outputs]\n#         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n#         batch_accs = [x['val_acc'] for x in outputs]\n#         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n#         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n#     def epoch_end(self, epoch, result):\n#         print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n#             epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n\n\ndef accuracy(probs, target):\n    predictions = probs.argmax(dim=1)\n    corrects = (predictions == target)\n    accuracy = corrects.sum().float() / float(target.size(0))\n    return accuracy\n\n\ndef train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    for batch in iterator:\n        optimizer.zero_grad()\n        text, text_lengths = batch.text\n#         # let's \n        predictions = model(text, text_lengths)\n#         predictions = model(batch.embedded)\n        loss = criterion(predictions, batch.target.squeeze()) #batch.target.squeeze()\n        acc = accuracy(predictions, batch.target)        \n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef run_train_and_val(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n    best_valid_loss = float('inf')\n\n    for epoch in range(epochs):\n\n        # train the model\n        train_loss, train_acc = train(model, train_iterator,    optimizer, criterion)\n\n        # evaluate the model\n        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n\n        # save the best model\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n        \ndef evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths = batch.text\n            predictions = model(text, text_lengths).squeeze(1)\n            loss = criterion(predictions, batch.target)\n            acc = accuracy(predictions, batch.target)\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_default_device()\n# loss_func = nn.CrossEntropyLoss()\n# model = CNN(max_vocab_size, embedding_size, n_filters, filter_sizes, pool_size, hidden_size, num_classes,\n#                     dropout_keep_prob)\n# cnn_model = to_device(model, device)\n# model_type = \"CNN\"\n# #cnn_model\n# optimizer = torch.optim.Adam(cnn_model.parameters(), lr=lr)\n# run_train_and_val(epochs, cnn_model, train_iterator, valid_iterator, optimizer, loss_func, model_type)\npath = \"./\"\ncnn_model.load_state_dict(torch.load(os.path.join(path, \"saved_weights_CNN.pt\")))\ntest_loss, test_acc = evaluate(cnn_model, valid_iterator, loss_func)\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jovian --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import jovian\njovian.commit(project=project_name, environment=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}